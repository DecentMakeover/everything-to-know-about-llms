## Transformers — Essential Papers

From the original transformer to efficient variants, scaling, and adaptation.

### Foundations
- **Attention Is All You Need (2017)** — Vaswani et al. [Paper](https://arxiv.org/abs/1706.03762)
- **The Annotated Transformer**: Implementation reference — Rush [Post](http://nlp.seas.harvard.edu/annotated-transformer/)

### Pretraining and architectures
- **BERT (2018)** — Devlin et al. [Paper](https://arxiv.org/abs/1810.04805)
- **GPT-1/2/3 (2018–2020)** — Radford et al., Brown et al. [GPT-3](https://arxiv.org/abs/2005.14165)
- **Transformer-XL (2019)** — Dai et al. [Paper](https://arxiv.org/abs/1901.02860)
- **XLNet (2019)** — Yang et al. [Paper](https://arxiv.org/abs/1906.08237)
- **RoBERTa (2019)** — Liu et al. [Paper](https://arxiv.org/abs/1907.11692)
- **ALBERT (2019)** — Lan et al. [Paper](https://arxiv.org/abs/1909.11942)
- **T5 (2020)** — Raffel et al. [Paper](https://arxiv.org/abs/1910.10683)
- **ELECTRA (2020)** — Clark et al. [Paper](https://arxiv.org/abs/2003.10555)

### Efficient/long-context transformers
- **Reformer (2020)** — Kitaev et al. [Paper](https://arxiv.org/abs/2001.04451)
- **Performer (2020)** — Choromanski et al. [Paper](https://arxiv.org/abs/2009.14794)
- **Linformer (2020)** — Wang et al. [Paper](https://arxiv.org/abs/2006.04768)
- **Longformer (2020)** — Beltagy et al. [Paper](https://arxiv.org/abs/2004.05150)
- **BigBird (2020)** — Zaheer et al. [Paper](https://arxiv.org/abs/2007.14062)
- **FlashAttention (2022)** — Dao et al. [Paper](https://arxiv.org/abs/2205.14135)
- **RoPE (2021)** — Su et al. [Paper](https://arxiv.org/abs/2104.09864)
- **ALiBi (2021)** — Press et al. [Paper](https://arxiv.org/abs/2108.12409)

### Vision and detection with transformers (context)
- **ViT (2020)** — Dosovitskiy et al. [Paper](https://arxiv.org/abs/2010.11929)
- **Swin Transformer (2021)** — Liu et al. [Paper](https://arxiv.org/abs/2103.14030)
- **DETR (2020)** — Carion et al. [Paper](https://arxiv.org/abs/2005.12872)

### Parameter-efficient finetuning
- **LoRA (2021)** — Hu et al. [Paper](https://arxiv.org/abs/2106.09685)
- **Prompt Tuning (2021)** — Lester et al. [Paper](https://arxiv.org/abs/2104.08691)

### Scaling laws for LMs
- **Scaling Laws for Neural Language Models (2020)** — Kaplan et al. [Paper](https://arxiv.org/abs/2001.08361)
- **Chinchilla: Training Compute-Optimal LLMs (2022)** — Hoffmann et al. [Paper](https://arxiv.org/abs/2203.15556)

Suggested path: Transformer → BERT/GPT → RoBERTa/T5 → Transformer-XL/XLNet → Longformer/BigBird → FlashAttention → LoRA.


